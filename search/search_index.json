{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":""},{"location":"#presto-hudi-workshop-building-an-open-data-lakehouse-with-presto-and-apache-hudi","title":"Presto-Hudi Workshop - Building an Open Data Lakehouse with Presto and Apache Hudi","text":"<p>Welcome to our workshop! In this workshop, you\u2019ll learn the basics of Presto, the open-source SQL query engine, and it's support for Hudi. You\u2019ll get Presto running locally on your machine and connect to an S3-based data source and a Hive metastore, which enables our Hudi integration. This is a beginner-level workshop for software developers and engineers who are new to Presto and Hudi. At the end of the workshop, you will understand how to integrate Presto with Hudi and MinIO and to understand Hudi's unique features.</p> <p>The goals of this workshop are to show you:</p> <ul> <li>What is Apache Hudi and how to use it</li> <li>How to connect Presto to MinIO s3 storage and an Hudi-compatible Hive metastore using Docker</li> <li>How to take advantage of Hudi using Presto and why you would want to</li> </ul>"},{"location":"#about-this-workshop","title":"About this workshop","text":"<p>The introductory page of the workshop is broken down into the following sections:</p> <ul> <li>Agenda</li> <li>Compatibility</li> <li>Technology Used</li> <li>Credits</li> </ul>"},{"location":"#agenda","title":"Agenda","text":"Introduction Introduction to the technologies used Pre-work Prerequisites for the workshop Lab 1: Set up an Open Lakehouse Set up Presto &amp; Spark clusters, a Hive metastore, and an s3 storage mechanism Lab 2: Create &amp; Query Basic Hudi Tables Set up a Hudi tables from the <code>spark-shell</code> and explore them in MinIO and Presto Lab 3: Explore Hudi Table &amp; Query Types Explore how to create and interact with different types of Hudi tables and queries (intermediate-level concepts)"},{"location":"#compatibility","title":"Compatibility","text":"<p>This workshop has been tested on the following platforms:</p> <ul> <li>Linux: Ubuntu 22.04</li> <li>MacOS: M1 Mac</li> </ul>"},{"location":"#technology-used","title":"Technology Used","text":"<ul> <li>Docker: A container engine to run several applications in self-contained containers.</li> <li>Presto: A fast and Reliable SQL Engine for Data Analytics and the Open Lakehouse</li> <li>Apache Hudi: A high-performance open table format to bring database functionality to your data lakes</li> <li>Spark: ! multi-language engine for executing data engineering, data science, and machine learning on single-node machines or clusters</li> <li>MinIO: A high-performance, S3 compatible object store</li> </ul>"},{"location":"#credits","title":"Credits","text":"<ul> <li>Kiersten Stokes</li> <li>Deepak Panda</li> </ul>"},{"location":"introduction/","title":"Introduction","text":"<p>A data lakehouse is a data platform that merges the best aspects of data warehouses and data lakes into one data management solution. If you are looking for an open-source solution for a data lakehouse, Presto is the perfect choice. Presto is a fast and reliable SQL query engine for data analytics on the open lakehouse. It has a wide variety of use cases, like running interactive/ad hoc queries at sub-second performance for your high-volume apps, or lengthy ETL jobs that aggregate or join terabytes of data. Presto is designed to be adaptive, flexible, and extensible. The plugin mechanism it provides allows you to connect to different data sources. A single Presto query can combine data from multiple sources, archiving analytics across your entire organization. Dozens of connectors are available from the Presto community today. Here is a high-level architecture diagram:</p> <p></p>"},{"location":"introduction/#data-lakehouses","title":"Data Lakehouses","text":"<p>In order to understand the data lakehouse, we should cover the other data storage solutions that it is based upon: the data warehouse and the data lake.</p> <ul> <li>Data warehouse: a storage system where relational (row- and column-based) tables are stored in order to perform fast analytics</li> <li>Data lake: a storage system where a great deal of semi-structured (XML, and JSON) or unstructured data (files, images, time-series, etc.) is stored as-is for a relatively low cost</li> </ul> <p>A data lakehouse combines the query speed and data quality of a data warehouse with the flexibility and low storage cost of a data lake. An open lakehouse has the additional benefit of being based on open technologies. A general diagram of the architecture of a data lakehouse is below.</p> <p></p> <p>As seen in the above diagram, Presto and Apache Hudi are open source projects that each have a place in the open data lakehouse. Let's look at where they fit in the ecosystem below.</p>"},{"location":"introduction/#presto-overview","title":"Presto Overview","text":"<p>Presto is a query engine, which is a piece of software that sits on top of the underlying data storage architecture and fulfills requests for data by optimizing the data retrieval process. More specifically, Presto is a distributed query engine for fast SQL-based analytics.</p> <p>Presto is flexible and supports querying across diverse sources, including both structured relational databases and unstructured and semi-structured NoSQL data sources. Presto uses what it calls 'connectors' to integrate with this wide range of external data sources. Any data source can be queried as long as the data source adapts to the API expected by Presto. This makes Presto extremely flexible and extensible. Likewise, Presto supports many different file formats, such as ORC, Avro, Parquet, CSV, JSON, and more. Presto also supports a few different table formats including Delta Lake and Apache Hudi.</p> <p>Presto was open-sourced in 2019 when it was donated to the Linux Foundation and is under the open source governance of the Presto Foundation. All of these reasons make it the perfect choice of query engine for an open data lakehouse.</p>"},{"location":"introduction/#table-formats","title":"Table Formats","text":"<p>In the above section, we mentioned that Presto supports multiple file formats and multiple table formats. Let's explore the meaning of these terms more closely. A file format is just the structure of a file that tells a program how to display its contents, as specified by the file extension. For example <code>.txt</code> is a file format. In the case of a data lakehouse, there are a few different file formats that can be used to store table data. Some popular options are Avro, Parquet, and Orc.</p> <p>A table format, on the other hand, is more like a metadata layer between the data files and whatever is trying to access the table that is represented by these files. A table format determines how the files that make up a table are organized and brings database-like features and efficiency to a data lake. In this way, it is a key part of a data lakehouse. Table formats support features that often aren't available on traditional data lakes, such as ACID transactions and row-level operations. The three most prevalent open table formats are Apache Iceberg, Apache Hudi, and Delta Lake. Take a look at the comparison table below:</p>"},{"location":"introduction/#apache-hudi","title":"Apache Hudi","text":"<p>Apache Hudi is a fast growing data lake storage system that helps organizations build and manage petabyte-scale data lakes. Hudi brings stream style processing to batch-like big data by introducing primitives such as upserts, deletes and incremental queries. These features help surface faster, fresher data on a unified serving layer. Hudi tables can be stored on the Hadoop Distributed File System (HDFS) or cloud stores and integrates well with popular query engines such as Presto, Apache Hive, Apache Spark and Apache Impala.</p> <p>Hudi enables storing vast amounts of data on top of existing DFS compatible storage while also enabling stream processing in addition to typical batch-processing. This is made possible by providing two new primitives. Specifically:</p> <ul> <li>Update/Delete Records: Hudi provides support for updating/deleting records, using fine grained file/record level indexes, while providing transactional guarantees for the write operation. Queries process the last such committed snapshot, to produce results.</li> <li>Change Streams: Hudi also provides first-class support for obtaining an incremental stream of all the records that were updated/inserted/deleted in a given table, from a given point-in-time, and unlocks a new incremental-query category.</li> </ul> <p>Hudi supports the following table types.</p> <ul> <li>Copy On Write (COW): Stores data using exclusively columnar file formats (e.g parquet). Updates version &amp; rewrites the files by performing a synchronous merge during write.</li> <li>Merge On Read (MOR): Stores data using file versions with combination of columnar (e.g parquet) + row based (e.g avro) file formats. Updates are logged to delta files &amp; later compacted to produce new versions of columnar files synchronously or asynchronously.</li> </ul> <p>These primitives and tables types allow Hudi to support multiple types of queries. Specifically,</p> <ul> <li>Snapshot Queries: Queries see the latest snapshot of the table as of a given commit or compaction action. In case of merge-on-read table, it exposes near-real time data (few mins) by merging the base and delta files of the latest file version on-the-fly. For copy-on-write tables, it provides a drop-in replacement for existing parquet tables, while providing upsert/delete and other write side features.</li> <li>Incremental Queries: Queries only see new data written to the table since a given commit/compaction. This effectively provides change streams to enable incremental data pipelines.</li> <li>Read Optimized Queries: Queries see the latest snapshot of a table as of a given commit/compaction action. Exposes only the base/columnar files in latest file versions and guarantees the same columnar query performance compared to a non-hudi columnar table.</li> </ul>"},{"location":"introduction/#getting-started","title":"Getting Started","text":"<p>In this workshop, you will use docker to spin up</p> <ul> <li>A Presto cluster consisting of a single server node</li> <li>A Hive metastore backed by a MySQL database</li> <li>A Spark cluster to create and populate the Hudi tables</li> <li>A MinIO s3-compatible storage instance</li> </ul> <p>Without further ado, let's get started.</p>"},{"location":"lab-1/","title":"Set up an Open Lakehouse","text":"<p>In this section, you will use docker compose to create an open lakehouse with the following components:</p> <ul> <li>A MinIO s3 Object Storage as the data storage component</li> <li>A Hive metastore to keep track of table metadata</li> <li>A single-node Presto cluster as the SQL query engine</li> <li>A Spark cluster to use to create Hudi tables</li> </ul> <p>This section is comprised of the following steps:</p> <ul> <li>Set up an Open Lakehouse</li> <li>1. Build the minimal Hive metastore image</li> <li>2. Spin up all containers<ul> <li>Lakehouse architecture</li> <li>Looking at the Docker Compose file</li> </ul> </li> <li>2. Check that services have started</li> <li>3. Connect Presto to Hudi</li> </ul>"},{"location":"lab-1/#1-build-the-minimal-hive-metastore-image","title":"1. Build the minimal Hive metastore image","text":"<p>In order to use Hudi with Presto, we have to set up a Hive metastore to sync our data across different database engines. We'll build a minimal Hive metastore image from the Dockerfile included in this repo.</p> <ol> <li>Open a terminal locally and run the following commands to build the Hive metastore image:</li> </ol> <pre><code>cd src/conf\ndocker compose build\n</code></pre> <p>You will see console output while the image builds. The build may take several minutes to complete. While we wait, let's go over some of the configuration options in the <code>metastore-site.xml</code> file that will be passed to the metastore container on startup. The following properties are of particular interest to us:</p> <ul> <li><code>metastore.thrift.uris</code>: defines the endpoint of the metastore service that we're using. The hostname supplied corresponds to the <code>hostname</code> that is assigned to our metastore container in <code>docker-compose.yml</code> (see below section). The port <code>9083</code> is the default port for the Hive metastore service. As with any URI, <code>thrift://</code> is the protocol by which communication takes place.</li> <li><code>javax.jdo.option.ConnectionURL</code>: defines the URL of the underlying database that supports the metastore service (this is different from the underlying source for our table data, which is MinIO/s3). The hostname is again the hostname of the MySQL database container (again defined in <code>docker-compose.yml</code>), and the port is the default MySQL port. We also give a path to a specific database, <code>metastore_db</code>, that will act the storage for our metastore</li> <li><code>javax.jdo.option.ConnectionUserName</code> and <code>javax.jdo.option.ConnectionPassword</code>: the username and password required to access the underlying MySQL database</li> <li><code>fs.s3a.endpoint</code>: the endpoint that provides the storage for the table data (not the metastore data). The hostname and port given follow the same convention as those mentioned earlier.</li> <li><code>fs.s3a.access.key</code> and <code>fs.s3a.secret.key</code>: the username and password required for the metastore to access the underlying table data</li> <li><code>fs.s3a.path.style.access</code>: we set this property to true to indicate that requests will be sent to, for example, <code>s3.example.com/bucket</code> instead of <code>bucket.s3.example.com</code></li> </ul> <p>Once the image has been built, we can move to step 2.</p> <ol> <li>Check that the <code>hive-metastore</code> image has been successfully created:</li> </ol> <pre><code>docker image list\n</code></pre> <p>You should see a <code>hive-metastore</code> image in your list of images, similar to this:</p> <pre><code> REPOSITORY                       TAG                           IMAGE ID         CREATED          SIZE\n hive-metastore              latest                        28377ad2303e     2 minutes ago    1.14GB\n</code></pre> <p>This means that the image has been created with the tag <code>latest</code>.</p>"},{"location":"lab-1/#2-spin-up-all-containers","title":"2. Spin up all containers","text":"<p>Bring up the necessary containers with the following command:</p> <pre><code>docker compose up -d\n</code></pre> <p>This command may take quite awhile to run, as docker has to pull an image for each container and start them. While we wait for startup to complete, let's cover some important background architecture for our lakehouse and see how it relates to the <code>docker-compose.yml</code> file. This file defines everything about our multi-container application.</p>"},{"location":"lab-1/#lakehouse-architecture","title":"Lakehouse architecture","text":"<p>Recall that, at minimum, a lakehouse consists of a processing engine, data stored in a lake format such as s3-compatible storage, and a table format to organize stored data into tables. The first two pieces are somewhat straigtforward for this workshop: we spin up a Presto container for processing and a MinIO container for storage. The table format we will use is Hudi - but how do we hook Hudi into our lakehouse? In this example, the Hudi metadata is synced by the Hive metastore.</p> <p>In order to use Hudi with both Presto and Spark, we have to ensure that they are both working off the most current data. We will register Hudi table with the Hive metastore when we create them in Spark by setting the <code>HIVE_SYNC_ENABLED_OPT_KEY</code> option to \"true\" and providing other required properties.</p>"},{"location":"lab-1/#looking-at-the-docker-compose-file","title":"Looking at the Docker Compose file","text":"<p>First, we define a network: <code>presto_network</code>. Each of our containers will communicate across this network.</p> <p>The next section is the <code>service</code> section, which is the bulk of the file. The first service we define is that of the Presto cluster, which we have named <code>coordinator</code>. We provide a human-readable <code>container_name</code> (also \"coordinator\") and the Docker <code>image</code> that we want this service to be based on, which is the <code>presto</code> image with tag <code>0.287</code> hosted in the <code>prestodb</code> DockerHub repository. The value <code>8000:8080</code> means that we want to map port 8000 on the Docker host (left side of the colon) to port 8080 in the container (right of the colon).</p> <p>We also need to supply the Presto container with some necessary configuration files, which we define using the <code>volume</code> key. Similar to how we defined the port, we're saying here that we want to map the files that are in the <code>presto/etc</code> directory (relative to our current working directory on the command line) to the location is the container corresponding to <code>/opt/presto-server/etc</code>, which is the directory that Presto expects to find configuration files. Here are the configuration settings for the Presto server as given in <code>./etc/config.properties</code> that we will pass to our server container:</p> <pre><code>coordinator=true\nnode-scheduler.include-coordinator=true\nhttp-server.http.port=8080\ndiscovery-server.enabled=true\ndiscovery.uri=http://localhost:8080\nnode.environment=test\n</code></pre> <ul> <li><code>coordinator</code>: defines whether this Presto server acts as a coordinator or not. Use value <code>true</code> for a coordinator</li> <li><code>node-scheduler.include-coordinator</code>: defines whether the Presto server acts as a worker as well as a coordinator. We use the value <code>true</code> to accept worker tasks since we only have one node in our Presto cluster</li> <li><code>http-server.http.port</code>: defines the port number for the HTTP server</li> <li><code>discovery-server.enabled</code>: defines whether the Presto server should act as a discovery server to register workers</li> <li><code>discovery.uri</code>: defines the discovery server's URI, which is itself in this case</li> <li><code>node.environment</code>: defines the name of the environment; all Presto nodes in a cluster must have the same environment name</li> </ul> <p>Next we specify any necesssary environment variables. In this case, we give the username and password required to access our MinIO storage. Finally, we state that this container is part of the previouly-created <code>presto_network</code>, meaning it will be able to communicate with other services on the network.</p> <p>Let's do the same for the <code>hive-metastore</code> service, which has a few lines we haven't seen yet. The <code>build</code> property is what allowed us to build the custom image located in the <code>hive-metastore</code> directory in the previous step. We'll specify the image that we just created as the <code>image</code> property value. We also give a <code>hostname</code> for this container, the value of which we supply in the <code>metastore-site.xml</code> configuration file, which itself is mapped to the appropriate location inside the container using the <code>volumes</code> property. The last property that we will call out is <code>depends_on</code>, which defines dependencies between our service containers. In this case, the <code>mysql</code> container will be started before the <code>hive-metastore</code> Presto container. This makes sense since the MySQL database needs to be running before the Hive metastore service can start.</p> <p>The <code>minio</code> container is our s3 service that we supply our access key and secret to. The <code>mc</code> container creates a bucket in our s3 storage (called <code>warehouse</code>) that will be where we eventually place our Hudi tables. These services don't require as much setup as the others.</p> <p>Finally, our <code>hudi-spark</code> service starts up a Spark cluster with a handful of required properties files and the jars that we downloaded in the pre-work steps.</p> <p>On the last few lines of the file, we define additional <code>volumes</code>. These are different from those that we created on the fly in the <code>services</code> section in that here we create named volumes that can be persisted even if some containers need to restart.</p> <p>The output of the <code>up</code> command will look like the below when all containers have been started:</p> <pre><code>[+] Running 9/9\n \u2714 Network src_presto-network   Created         0.0s\n \u2714 Volume \"src_minio-data\"      Created         0.0s\n \u2714 Volume \"src_mysql-data\"      Created         0.0s\n \u2714 Container coordinator        Started         1.0s\n \u2714 Container mysql              Started         1.0s\n \u2714 Container minio              Started         1.0s\n \u2714 Container mc                 Started         1.0s\n \u2714 Container hive-metastore     Started         1.4s\n \u2714 Container spark              Started         1.3s\n</code></pre>"},{"location":"lab-1/#2-check-that-services-have-started","title":"2. Check that services have started","text":"<p>Let's also check that our relevant services have started.</p> <pre><code>docker logs --tail 100 minio\n</code></pre> <p>If started successfully, the logs for the <code>minio</code> container should include something similar to the below:</p> <pre><code>API: http://172.23.0.4:9090  http://127.0.0.1:9090 \nWebUI: http://172.23.0.4:9091 http://127.0.0.1:9091 \n</code></pre> <p>We will be using the console address in the next exercise. Let's check that the Hive metastore is running with the following command:</p> <pre><code>docker logs --tail 50 hive-metastore\n</code></pre> <p>If the metastore service is up and running properly, you should see the below lines somewhere near the bottom of the logs, likely interspersed with other logging information.</p> <pre><code>...\nInitialization script completed\nschemaTool completed\n...\n2023-11-20 23:21:56: Starting Metastore Server\n...\n</code></pre> <p>If the Hive metastore is up, the MySQL database also must be up because the metastore requires this on startup. If you do not see the above success messages and instead see a lot of errors in the logs for the Hive metastore, try re-starting the container:</p> <pre><code>docker restart hive-metastore\n</code></pre> <p>Now, let's check the Presto node:</p> <pre><code>docker logs --tail 100 coordinator\n</code></pre> <p>If the Presto server is up and running properly, the last lines of the output would like the following:</p> <pre><code>2023-11-14T04:03:22.246Z        INFO    main    com.facebook.presto.storage.TempStorageManager  -- Loading temp storage local --\n2023-11-14T04:03:22.251Z        INFO    main    com.facebook.presto.storage.TempStorageManager  -- Loaded temp storage local --\n2023-11-14T04:03:22.256Z        INFO    main    com.facebook.presto.server.PrestoServer ======== SERVER STARTED ========\n</code></pre> <p>The Presto server will likely take the longest to start up. If you don't see any errors or the <code>SERVER STARTED</code> notice, wait a few minutes and check the logs again.</p> <p>You can also assess the status of your cluster using the Presto UI at the relevant IP address: <code>http://&lt;your_ip&gt;:8080</code>. If you're running everything on your local machine, the address will be http://localhost:8000. You should see 1 active worker (which is the coordinator node, in our case) and a green \"ready\" status in the top right corner, as seen below.</p> <p></p>"},{"location":"lab-1/#3-connect-presto-to-hudi","title":"3. Connect Presto to Hudi","text":"<p>Our containers are up and running, but you may be wondering how Presto works with Hudi, as we didn't see any special key-value pairs for this in the docker compose file. Presto makes it very easy to get started with Hudi, with no need to install any additional packages. If we started the Presto CLI right now, we would be able to query any existing tables synced to the Hive metastore in Hudi format - but how? Recall the volume that we passed to the <code>coordinator</code> container. This volume includes a directory called <code>catalog</code> that was mapped to the <code>/opt/presto-server/etc/catalog</code> location in the container along with the other server configuration files. The <code>catalog</code> directory is where the Presto server looks to see what underlying data sources should be made available to Presto and how to connect to those sources. Let's take a look at the <code>hudi.properties</code> file that was mapped to the Presto cluster.</p> <pre><code>connector.name=hudi\nhive.metastore.uri=thrift://hive-metastore:9083\nhive.s3.path-style-access=true\nhive.s3.endpoint=http://minio:9090\nhive.s3.aws-access-key=minio\nhive.s3.aws-secret-key=minio123\nhive.non-managed-table-writes-enabled=true\nhive.copy-on-first-write-configuration-enabled=false\n</code></pre> <p>This file includes a required <code>connector.name</code> property that indicates we're defining properties for a Hudi connector. The remaining configuration options give the details needed in order to access our underlying s3 data source and the metadata for these tables stored in the Hive metastore. When Presto starts, it accesses these configuration files in order to determine which connections it can make.</p> <p>Leveraging high-performance huge-data analytics is as easy as that! Let's move to the next exercise to set up our data source and start creating some tables.</p>"},{"location":"lab-2/","title":"Create and Query Basic Hudi Tables","text":"<p>In this section, you will create basic Hudi tables with Spark and query them with Presto.</p> <p>This section is comprised of the following steps:</p> <ul> <li>Create and Query Basic Hudi Tables</li> <li>1. Create Hudi tables</li> <li>2. Query table with Presto</li> <li>3. Add data to table and query<ul> <li>Optional shutdown</li> </ul> </li> </ul>"},{"location":"lab-2/#1-create-hudi-tables","title":"1. Create Hudi tables","text":"<p>In this section we'll explore Hudi tables. Currently, it is not possible to create Hudi tables from Presto, so we will use Spark to create our tables. To do so, we'll enter the Spark container and start the <code>spark-shell</code>:</p> <pre><code>docker exec -it spark /opt/spark/bin/spark-shell\n</code></pre> <p>It may take a few moments to initialize before you see the <code>scala&gt;</code> prompt, indicating that the shell is ready to accept commands. Enter \"paste\" mode by typing the following and pressing enter:</p> <pre><code>:paste\n</code></pre> <p>For example:</p> <pre><code>scala&gt; :paste\n\n// Entering paste mode (ctrl-D to finish)\n</code></pre> <p>Copy and paste the below code, which imports required packages, creates a Spark session, and defines some variables that we will reference in subsequent code.</p> <pre><code>import org.apache.spark.sql.{SparkSession, SaveMode}\nimport scala.util.Random\nimport java.util.UUID\n\nval spark = SparkSession.builder()\n  .appName(\"HudiToMinIO\")\n  .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n  .config(\"spark.sql.catalogImplementation\", \"hive\")\n  .config(\"hive.metastore.uris\", \"thrift://hive-metastore:9083\")\n  .config(\"spark.sql.hive.convertMetastoreParquet\", \"false\")\n  .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\")\n  .config(\"spark.hadoop.fs.s3a.access.key\", \"minio\")\n  .config(\"spark.hadoop.fs.s3a.secret.key\", \"minio123\")\n  .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n  .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n  .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n  .enableHiveSupport()\n  .getOrCreate()\n\nimport spark.implicits._\nimport org.apache.hudi.QuickstartUtils._\nimport scala.collection.JavaConversions._\nimport org.apache.spark.sql.SaveMode._\nimport org.apache.hudi.DataSourceReadOptions._\nimport org.apache.hudi.DataSourceWriteOptions._\nimport org.apache.hudi.config.HoodieWriteConfig._\n\nval basePath = \"s3a://warehouse/hudi-tables\"\nval dbName = \"default\"\n\ndef hudiOptions(tableName: String, tableType: String, precombineField: String, partitionField: Option[String] = None): Map[String, String] = {\n  Map(\n    \"hoodie.table.name\" -&gt; tableName,\n    \"hoodie.datasource.write.recordkey.field\" -&gt; \"uuid\",\n    \"hoodie.datasource.write.precombine.field\" -&gt; precombineField,\n    \"hoodie.datasource.write.table.name\" -&gt; tableName,\n    \"hoodie.datasource.write.operation\" -&gt; \"upsert\",\n    \"hoodie.datasource.write.table.type\" -&gt; tableType,\n    \"hoodie.datasource.write.hive_style_partitioning\" -&gt; \"true\",\n    \"hoodie.datasource.hive_sync.enable\" -&gt; \"true\",\n    \"hoodie.datasource.hive_sync.mode\" -&gt; \"hms\",\n    \"hoodie.datasource.hive_sync.database\" -&gt; dbName,\n    \"hoodie.datasource.hive_sync.table\" -&gt; tableName,\n  ) ++ partitionField.map(f =&gt; Map(\"hoodie.datasource.write.partitionpath.field\" -&gt; f)).getOrElse(Map.empty)\n}\nval dataGen = new DataGenerator\n</code></pre> <p>Make sure you include a newline character at the very end. Press <code>Ctrl+D</code> to begin executing the pasted code.</p> <p>We will complete the same process with our next code block, which will create and populate our table with randomly generated data about taxi trips. Here we specify that we want the table to be a \"Copy on Write\" table, but this is not strictly necessary - this is the default table type in Hudi.  Notice that we are including an extra column, <code>commit_num</code> that will show us the commit in which any given row was added.</p> <pre><code>val inserts = convertToStringList(dataGen.generateInserts(50))\nval data = spark.read.json(spark.sparkContext.parallelize(inserts, 2))\n\nval tableName = \"trips_table\"\n\ndata.withColumn(\"commit_num\", lit(\"update1\")).write.format(\"hudi\").\n    options(getQuickstartWriteConfigs).\n    options(hudiOptions(tableName, \"COPY_ON_WRITE\", \"ts\")).\n    mode(Overwrite).\n    save(s\"$basePath/$tableName\");\n</code></pre> <p>Before we go on to query these tables, let's take a look at what files and directories have been created for this table in our s3 storage. Go to MinIO UI http://localhost:9091 and log in with the username and password that we defined in <code>docker-compose.yaml</code> (<code>minio</code>/<code>minio123</code>). Under the <code>hudi-tables</code> path, there should be a single sub-path called <code>trips_table</code>. Below is a look at the file structure.</p> <p></p> <p>We have one parquet data file, a Hudi-specific metadata file, and another sub-path, <code>.hoodie</code>. The latter is where Hudi keeps most of the metadata for the <code>trips_table</code>, including the commit history. We can see that there is one set of <code>commit</code> files created to keep track of the initial data we've inserted into the table. Feel free to explore these files further.</p> <p></p>"},{"location":"lab-2/#2-query-table-with-presto","title":"2. Query table with Presto","text":"<p>Now let's query this table with Presto. In a new terminal tab or window, exec into the Presto container and start the Presto CLI to query our table.</p> <pre><code> docker exec -it coordinator presto-cli\n</code></pre> <p>We first specify that we want to use the Hudi catalog and <code>default</code> schema for all queries here on out. The 'default' schema has been implicitly created for us in Presto because it already exists in the Hive metastore. Then, execute a <code>show tables</code> command:</p> <pre><code>use hudi.default;\n</code></pre> <p>For example:</p> <pre><code>presto&gt; use hudi.default;\nUSE\n</code></pre> <p>Next, list the available tables:</p> <pre><code>show tables;\n</code></pre> <p>For example:</p> <pre><code>presto:default&gt; show tables;\n       Table        \n--------------------\n trips_table        \n(1 row)\n</code></pre> <p>As expected, we see a single table here with the name of the table we created in the <code>spark-shell</code>. We can run a <code>select *</code> on this data.</p> <pre><code>select * from trips_table limit 10;\n</code></pre> <p>For example:</p> <pre><code>presto:default&gt; select * from trips_table limit 10;\n</code></pre> <p>Note</p> <p>Note: Press \"q\" to exit out out after you see \"(END)\"</p> <p>The results (omitted here for space) show more than you may expect to see with a traditional table. The columns prefixed with <code>_hoodie</code> are metadata properties that Hudi uses to manage the table state. You can also see the original columns at far right of the table (<code>begin_lat</code>, <code>begin_lon</code>, <code>fare</code>, etc.).</p> <p>Let's execute a query that pares down these results slightly:</p> <pre><code>select _hoodie_commit_time, commit_num, _hoodie_file_name, fare, begin_lon, begin_lat from trips_table order by _hoodie_commit_time;\n</code></pre> <p>For example:</p> <pre><code>presto:default&gt; select _hoodie_commit_time, commit_num, _hoodie_file_name, fare, begin_lon, begin_lat from trips_table order by _hoodie_commit_time;\n _hoodie_commit_time | commit_num |                             _hoodie_file_name                              |        fare        |      begin_lon       |      begin_lat       \n---------------------+------------+----------------------------------------------------------------------------+--------------------+----------------------+----------------------\n 20250729020557937   | update1    | df001f68-16bc-4289-b8f5-755e27aa9cd9-0_0-198-207_20250729020557937.parquet |  5.064924039635443 |   0.3329328066900805 |  0.05829265790920446 \n 20250729020557937   | update1    | df001f68-16bc-4289-b8f5-755e27aa9cd9-0_0-198-207_20250729020557937.parquet |  84.00133794186554 |   0.7041966710545763 |   0.3988839560181455 \n 20250729020557937   | update1    | df001f68-16bc-4289-b8f5-755e27aa9cd9-0_0-198-207_20250729020557937.parquet |  26.57512568578202 |   0.4130226064649045 |  0.14944324629969385 \n 20250729020557937   | update1    | df001f68-16bc-4289-b8f5-755e27aa9cd9-0_0-198-207_20250729020557937.parquet |  61.44682955106423 |  0.13477337728703764 | 0.025339371609693573 \n 20250729020557937   | update1    | df001f68-16bc-4289-b8f5-755e27aa9cd9-0_0-198-207_20250729020557937.parquet | 47.886740443749254 |  0.48671796674575485 |   0.6158099753827585 \n 20250729020557937   | update1    | df001f68-16bc-4289-b8f5-755e27aa9cd9-0_0-198-207_20250729020557937.parquet |  74.20408022629255 |   0.8293284066906385 |  0.10155019269585785 \n 20250729020557937   | update1    | df001f68-16bc-4289-b8f5-755e27aa9cd9-0_0-198-207_20250729020557937.parquet | 43.160518375539205 |   0.8898173709112844 |   0.9937697164202349 \n 20250729020557937   | update1    | df001f68-16bc-4289-b8f5-755e27aa9cd9-0_0-198-207_20250729020557937.parquet | 55.642805813402006 |   0.9727806198646577 |  0.19982748870912004 \n\n (additional rows omitted here...)\n</code></pre>"},{"location":"lab-2/#3-add-data-to-table-and-query","title":"3. Add data to table and query","text":"<p>Now, let's go back to our <code>spark-shell</code> terminal tab and add more data to our tables using paste mode. Note that our <code>commit_num</code> column value has changed.</p> <p>Note</p> <p>Note: If you exit your spark-shell and have to issue another <code>docker exec -it spark /opt/spark/bin/spark-shell</code> to restart it, you need to get into paste mode and reimport all the required packages again as you did above in step 1.</p> <pre><code>val updates = convertToStringList(dataGen.generateUpdates(50))\nval updatedData = spark.read.json(spark.sparkContext.parallelize(updates, 2));\n\nupdatedData.withColumn(\"commit_num\", lit(\"update2\")).write.format(\"hudi\").\n    options(getQuickstartWriteConfigs).\n    options(hudiOptions(tableName, \"COPY_ON_WRITE\", \"ts\")).\n    mode(Append).\n    save(s\"$basePath/$tableName\");\n</code></pre> <p>Now if we query the tables in the Presto CLI, we see that there are multiple commit times, as shown by our first two columns in the below query. Make sure to hold <code>Return</code> in order to see all 100 rows of the updated table.</p> <pre><code>select _hoodie_commit_time, commit_num, _hoodie_file_name, fare, begin_lon, begin_lat from trips_table order by _hoodie_commit_time;\n</code></pre> <p>For example:</p> <pre><code>presto:default&gt; select _hoodie_commit_time, commit_num, _hoodie_file_name, fare, begin_lon, begin_lat from trips_table order by _hoodie_commit_time;\n _hoodie_commit_time | commit_num |                             _hoodie_file_name                              |        fare        |      begin_lon       |      begin_lat       \n---------------------+------------+----------------------------------------------------------------------------+--------------------+----------------------+----------------------\n 20250729020557937   | update1    | dd513af1-2291-4fea-a588-06e7c55fec23-0_0-50-53_20250729230015275.parquet | 25.200733895704396 |   0.9805032518130713 |   0.3250925842689032 \n 20250729020557937   | update1    | dd513af1-2291-4fea-a588-06e7c55fec23-0_0-50-53_20250729230015275.parquet |  80.22532741048674 |  0.23558979258614088 |   0.9106955060439053 \n 20250729020557937   | update1    | dd513af1-2291-4fea-a588-06e7c55fec23-0_0-50-53_20250729230015275.parquet |  84.00133794186554 |   0.7041966710545763 |   0.3988839560181455 \n 20250729020557937   | update1    | dd513af1-2291-4fea-a588-06e7c55fec23-0_0-50-53_20250729230015275.parquet | 27.793208764156986 |   0.9676738239915396 |   0.5075033998567434 \n 20250729020557937   | update1    | dd513af1-2291-4fea-a588-06e7c55fec23-0_0-50-53_20250729230015275.parquet |  61.44682955106423 |  0.13477337728703764 | 0.025339371609693573 \n 20250729020557937   | update1    | dd513af1-2291-4fea-a588-06e7c55fec23-0_0-50-53_20250729230015275.parquet |  90.97255481383131 |   0.7196037664723752 |   0.9539455886006297 \n 20250729020557937   | update1    | dd513af1-2291-4fea-a588-06e7c55fec23-0_0-50-53_20250729230015275.parquet |  51.61325440476856 |  0.36749492639453507 |   0.7666637026082733 \n\n (additional rows omitted here...)\n 20250729230015275   | update2    | dd513af1-2291-4fea-a588-06e7c55fec23-0_0-50-53_20250729230015275.parquet |   48.68719058185585 |   0.4795784679677898 |  0.25038172085518196 \n 20250729230015275   | update2    | dd513af1-2291-4fea-a588-06e7c55fec23-0_0-50-53_20250729230015275.parquet |   75.48568723276352 |  0.15381057911655593 |   0.9767478911589815 \n 20250729230015275   | update2    | dd513af1-2291-4fea-a588-06e7c55fec23-0_0-50-53_20250729230015275.parquet |   45.81290455568438 |  0.02795404977976812 |    0.712778116485955 \n 20250729230015275   | update2    | dd513af1-2291-4fea-a588-06e7c55fec23-0_0-50-53_20250729230015275.parquet |  57.516128683731594 |   0.9507437299992999 |    0.651681577874527 \n 20250729230015275   | update2    | dd513af1-2291-4fea-a588-06e7c55fec23-0_0-50-53_20250729230015275.parquet |    77.6244653745167 |   0.6935649065845402 |  0.49277090579957905 \n 20250729230015275   | update2    | dd513af1-2291-4fea-a588-06e7c55fec23-0_0-50-53_20250729230015275.parquet | 0.09544772278234914 |   0.6651362795967237 |   0.8866112536048135 \n 20250729230015275   | update2    | dd513af1-2291-4fea-a588-06e7c55fec23-0_0-50-53_20250729230015275.parquet |   94.61601725825764 |  0.02466985290858581 |     0.53126878881263 \n</code></pre> <p>We can see that the values in the <code>_hoodie_file_name</code> have changed for the entries from the first commit. This is to be expected, as we created a \"Copy on Write\" table (the default table type in Hudi). We can also look in the MinIO UI again to see the different files that have been created. Notice that there is a new data file that holds the data added in the most recent commit as well as the previous commit's data as well.</p> <p></p> <p>In the <code>.hoodie</code> directory, we can also see additional commit files that have been added since the last time we checked. At this point, feel free to continue executing queries and adding data as desired to get familiar with Hudi functionality. You may also move on to lab 3.</p>"},{"location":"lab-2/#optional-shutdown","title":"Optional shutdown","text":"<p>If you do not intend to go on to lab 3 right now, you can shut down your lakehouse cluster with the following command:</p> <pre><code>docker compose down -v\n</code></pre> <p>This command will stop all containers and remove the volumes. You can omit the <code>-v</code> parameter if you want to keep your existing data for the <code>trips_table</code> in your MinIO storage and come back to complete lab 3 later.</p>"},{"location":"lab-3/","title":"Explore Hudi Table &amp; Query Types","text":"<p>In this section, you will create different types of Hudi tables with Spark and query them with Presto.</p> <p>This section is comprised of the following steps:</p> <ul> <li>Explore Hudi Table \\&amp; Query Types</li> <li>1. Create MoR Hudi table</li> <li>2. Query MoR table with Presto</li> <li>3. Add data to MoR table and query</li> <li>4. Create partitioned CoW Hudi table</li> <li>5. Query CoW table with Presto<ul> <li>Shutdown</li> </ul> </li> </ul> <p>If you previously stopped your lakehouse containers, restart them now with:</p> <pre><code>docker compose up -d\n</code></pre>"},{"location":"lab-3/#1-create-mor-hudi-table","title":"1. Create MoR Hudi table","text":"<p>In this section we'll explore Hudi Merge-on-Read (MoR) tables. MoR tables store data using file versions with combination of columnar (e.g parquet) + row based (e.g avro) file formats. Updates are logged to delta files &amp; later compacted to produce new versions of columnar files synchronously or asynchronously. Currently, it is not possible to create Hudi tables from Presto, so we will use Spark to create our tables. To do so, we'll enter the Spark container and start the <code>spark-shell</code>:</p> <pre><code>docker exec -it spark /opt/spark/bin/spark-shell\n</code></pre> <p>It may take a few moments to initialize before you see the <code>scala&gt;</code> prompt, indicating that the shell is ready to accept commands. Enter \"paste\" mode by typing the following and pressing enter:</p> <pre><code>:paste\n</code></pre> <p>For example:</p> <pre><code>scala&gt; :paste\n\n// Entering paste mode (ctrl-D to finish)\n</code></pre> <p>Copy and paste the below code, which imports required packages, creates a Spark session, and defines some variables that we will reference in subsequent code.</p> <p>Note</p> <p>Note: you do not need to copy/paste the below code if you completed lab 2 and have not restarted your <code>spark-shell</code>, but, regardless, doing so should be harmless.</p> <pre><code>import org.apache.spark.sql.{SparkSession, SaveMode}\nimport scala.util.Random\nimport java.util.UUID\n\nval spark = SparkSession.builder()\n  .appName(\"HudiToMinIO\")\n  .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n  .config(\"spark.sql.catalogImplementation\", \"hive\")\n  .config(\"hive.metastore.uris\", \"thrift://hive-metastore:9083\")\n  .config(\"spark.sql.hive.convertMetastoreParquet\", \"false\")\n  .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\")\n  .config(\"spark.hadoop.fs.s3a.access.key\", \"minio\")\n  .config(\"spark.hadoop.fs.s3a.secret.key\", \"minio123\")\n  .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n  .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n  .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n  .enableHiveSupport()\n  .getOrCreate()\n\nimport spark.implicits._\nimport org.apache.hudi.QuickstartUtils._\nimport scala.collection.JavaConversions._\nimport org.apache.spark.sql.SaveMode._\nimport org.apache.hudi.DataSourceReadOptions._\nimport org.apache.hudi.DataSourceWriteOptions._\nimport org.apache.hudi.config.HoodieWriteConfig._\n\nval basePath = \"s3a://warehouse/hudi-tables\"\nval dbName = \"default\"\n\ndef hudiOptions(tableName: String, tableType: String, precombineField: String, partitionField: Option[String] = None): Map[String, String] = {\n  Map(\n    \"hoodie.table.name\" -&gt; tableName,\n    \"hoodie.datasource.write.recordkey.field\" -&gt; \"uuid\",\n    \"hoodie.datasource.write.precombine.field\" -&gt; precombineField,\n    \"hoodie.datasource.write.table.name\" -&gt; tableName,\n    \"hoodie.datasource.write.operation\" -&gt; \"upsert\",\n    \"hoodie.datasource.write.table.type\" -&gt; tableType,\n    \"hoodie.datasource.write.hive_style_partitioning\" -&gt; \"true\",\n    \"hoodie.datasource.hive_sync.enable\" -&gt; \"true\",\n    \"hoodie.datasource.hive_sync.mode\" -&gt; \"hms\",\n    \"hoodie.datasource.hive_sync.database\" -&gt; dbName,\n    \"hoodie.datasource.hive_sync.table\" -&gt; tableName,\n  ) ++ partitionField.map(f =&gt; Map(\"hoodie.datasource.write.partitionpath.field\" -&gt; f)).getOrElse(Map.empty)\n}\n</code></pre> <p>Make sure you include a newline character at the very end. Press <code>Ctrl+D</code> to begin executing the pasted code.</p> <p>We will complete the same process with our next code block, which will create and populate our MoR table with randomly generated data about taxi trips. Notice that we are including an extra column, <code>commit_num</code> that will show us the commit in which any given row was added.</p> <pre><code>val dataGen = new DataGenerator\nval inserts = convertToStringList(dataGen.generateInserts(10))\nval data = spark.read.json(spark.sparkContext.parallelize(inserts, 2))\n\nval morTableName = \"mor_trips_table\"\n\ndata.withColumn(\"commit_num\", lit(\"update1\")).write.format(\"hudi\").\n    options(getQuickstartWriteConfigs).\n    options(hudiOptions(morTableName, \"MERGE_ON_READ\", \"ts\")).\n    mode(Overwrite).\n    save(s\"$basePath/$morTableName\");\n</code></pre> <p>Before we go on to query these tables, let's take a look at what files and directories have been created for this table in our s3 storage. Go to MinIO UI http://localhost:9091 and log in with the username and password that we defined in <code>docker-compose.yaml</code> (<code>minio</code>/<code>minio123</code>). Under the <code>hudi-tables</code> path, there should be a sub-path called <code>mor_trips_table</code>. Click into this path and explore the created files and directory structure, especially those in the <code>.hoodie</code> directory. This is where Hudi keeps metadata for the <code>mor_trips_table</code>. We can see that there is one set of <code>deltacommit</code> files created to keep track of the initial data we've inserted into the table.</p> <p></p>"},{"location":"lab-3/#2-query-mor-table-with-presto","title":"2. Query MoR table with Presto","text":"<p>Now let's query these tables with Presto. In a new terminal tab or window, exec into the Presto container and start the Presto CLI to query our table.</p> <pre><code> docker exec -it coordinator presto-cli\n</code></pre> <p>We first specify that we want to use the Hudi catalog and <code>default</code> schema for all queries here on out. Then, execute a <code>show tables</code> command:</p> <pre><code>use hudi.default;\n</code></pre> <p>For example:</p> <pre><code>presto&gt; use hudi.default;\nUSE\n</code></pre> <p>Next, list the available tables:</p> <pre><code>show tables;\n</code></pre> <p>For example:</p> <pre><code>presto:default&gt; show tables;\n       Table        \n--------------------\n mor_trips_table_ro \n mor_trips_table_rt \n trips_table        \n(3 rows)\n</code></pre> <p>Note</p> <p>You may not have the <code>trips_table</code> depending on when you completed lab 2 and if you shut down or restarted your lakehouse cluster.</p> <p>Notice how Hudi is implicity showing us two versions of the MoR table - one suffixed with <code>_ro</code> for \"read-optimized\" and one suffixed with <code>_rt</code> for \"real-time\". As expected, each provides a different view. Right now, querying them shows the same information since we've only inserted data into the table once at time of creation. Run the below query on both tables to verify this.</p> <pre><code>select _hoodie_commit_time, commit_num, _hoodie_file_name, fare, begin_lon, begin_lat from mor_trips_table_ro order by _hoodie_commit_time;\n</code></pre> <p>For example:</p> <pre><code>presto:default&gt; select _hoodie_commit_time, commit_num, _hoodie_file_name, fare, begin_lon, begin_lat from mor_trips_table_ro order by _hoodie_commit_time;\n _hoodie_commit_time | commit_num |                             _hoodie_file_name                              |        fare        |      begin_lon      |      begin_lat      \n---------------------+------------+----------------------------------------------------------------------------+--------------------+---------------------+---------------------\n 20250729021205937   | update1    | f226d78a-a109-4b0a-b9a6-e522b75c1037-0_0-256-264_20250729021205937.parquet |  60.34776367964548 | 0.04838041157380535 |  0.8043080489965999 \n 20250729021205937   | update1    | f226d78a-a109-4b0a-b9a6-e522b75c1037-0_0-256-264_20250729021205937.parquet |     77.87647707179 | 0.48628479602261987 |  0.5108935227213224 \n 20250729021205937   | update1    | f226d78a-a109-4b0a-b9a6-e522b75c1037-0_0-256-264_20250729021205937.parquet |  92.66658188648728 | 0.19250097971954727 | 0.29883236260196766 \n 20250729021205937   | update1    | f226d78a-a109-4b0a-b9a6-e522b75c1037-0_0-256-264_20250729021205937.parquet |  87.60866855274442 |  0.7072482413115851 |  0.4050368427403227 \n 20250729021205937   | update1    | f226d78a-a109-4b0a-b9a6-e522b75c1037-0_0-256-264_20250729021205937.parquet |  2.542602090419077 | 0.32513116973946665 |  0.5223409231279434 \n 20250729021205937   | update1    | f226d78a-a109-4b0a-b9a6-e522b75c1037-0_0-256-264_20250729021205937.parquet |  64.50549995709942 |   0.644801818366808 | 0.27416484281668874 \n 20250729021205937   | update1    | f226d78a-a109-4b0a-b9a6-e522b75c1037-0_0-256-264_20250729021205937.parquet |     87.90342171962 |  0.6392049453784305 |   0.988424424914435 \n 20250729021205937   | update1    | f226d78a-a109-4b0a-b9a6-e522b75c1037-0_0-256-264_20250729021205937.parquet |  4.683013253665058 |  0.9524155201748359 |  0.5358882850214233 \n 20250729021205937   | update1    | f226d78a-a109-4b0a-b9a6-e522b75c1037-0_0-256-264_20250729021205937.parquet |  52.45373021236392 | 0.16908935729553864 |  0.5977389933638982 \n 20250729021205937   | update1    | f226d78a-a109-4b0a-b9a6-e522b75c1037-0_0-256-264_20250729021205937.parquet | 63.153644129279854 | 0.08661048702300178 |  0.3591754879402138 \n(10 rows)\n</code></pre>"},{"location":"lab-3/#3-add-data-to-mor-table-and-query","title":"3. Add data to MoR table and query","text":"<p>Now, let's go back to our <code>spark-shell</code> terminal tab and add more data to our tables using paste mode. Note that our <code>commit_num</code> column value has changed.</p> <pre><code>val updates = convertToStringList(dataGen.generateUpdates(10))\nval updatedData = spark.read.json(spark.sparkContext.parallelize(updates, 2));\n\nupdatedData.withColumn(\"commit_num\", lit(\"update2\")).write.format(\"hudi\").\n    options(getQuickstartWriteConfigs).\n    options(hudiOptions(morTableName, \"MERGE_ON_READ\", \"ts\")).\n    mode(Append).\n    save(s\"$basePath/$morTableName\");\n</code></pre> <p>Now if we query the tables in the Presto CLI, we see that the MoR <code>RO</code> (\"read-optimized\") and <code>RT</code> (\"real-time\") tables are starting to look different. As you can guess by the names, the RT table has the freshest data, and the RO table still shows our previous state.</p> <pre><code>select _hoodie_commit_time, commit_num, _hoodie_file_name, fare, begin_lon, begin_lat from mor_trips_table_rt order by _hoodie_commit_time;\n</code></pre> <p>For example:</p> <pre><code>presto:default&gt; select _hoodie_commit_time, commit_num, _hoodie_file_name, fare, begin_lon, begin_lat from mor_trips_table_rt order by _hoodie_commit_time;\n _hoodie_commit_time | commit_num |                             _hoodie_file_name                              |        fare        |      begin_lon      |      begin_lat      \n---------------------+------------+----------------------------------------------------------------------------+--------------------+---------------------+---------------------\n 20250729021205937   | update1    | f226d78a-a109-4b0a-b9a6-e522b75c1037-0_0-256-264_20250729021205937.parquet |  92.66658188648728 | 0.19250097971954727 | 0.29883236260196766 \n 20250729021205937   | update1    | f226d78a-a109-4b0a-b9a6-e522b75c1037-0_0-256-264_20250729021205937.parquet |     77.87647707179 | 0.48628479602261987 |  0.5108935227213224 \n 20250729021205937   | update1    | f226d78a-a109-4b0a-b9a6-e522b75c1037-0_0-256-264_20250729021205937.parquet |  87.60866855274442 |  0.7072482413115851 |  0.4050368427403227 \n 20250729021205937   | update1    | f226d78a-a109-4b0a-b9a6-e522b75c1037-0_0-256-264_20250729021205937.parquet |  2.542602090419077 | 0.32513116973946665 |  0.5223409231279434 \n 20250729021205937   | update1    | f226d78a-a109-4b0a-b9a6-e522b75c1037-0_0-256-264_20250729021205937.parquet |     87.90342171962 |  0.6392049453784305 |   0.988424424914435 \n 20250729021205937   | update1    | f226d78a-a109-4b0a-b9a6-e522b75c1037-0_0-256-264_20250729021205937.parquet |  52.45373021236392 | 0.16908935729553864 |  0.5977389933638982 \n 20250729021301565   | update2    | f226d78a-a109-4b0a-b9a6-e522b75c1037-0                                     |   99.1562254763212 |  0.6294358584439047 |  0.8543808877516004 \n 20250729021301565   | update2    | f226d78a-a109-4b0a-b9a6-e522b75c1037-0                                     |  97.86876579077843 |   0.895279012685712 |  0.2650495107524782 \n 20250729021301565   | update2    | f226d78a-a109-4b0a-b9a6-e522b75c1037-0                                     | 15.893431524875934 | 0.22687250146427174 | 0.01766360374572995 \n 20250729021301565   | update2    | f226d78a-a109-4b0a-b9a6-e522b75c1037-0                                     |  11.38881031161545 |  0.8233625060614379 |   0.912094209732618 \n(10 rows)\n</code></pre> <p>And</p> <pre><code>select _hoodie_commit_time, commit_num, _hoodie_file_name, fare, begin_lon, begin_lat from mor_trips_table_ro order by _hoodie_commit_time;\n</code></pre> <p>For example:</p> <pre><code>presto:default&gt; select _hoodie_commit_time, commit_num, _hoodie_file_name, fare, begin_lon, begin_lat from mor_trips_table_ro order by _hoodie_commit_time;\n _hoodie_commit_time | commit_num |                             _hoodie_file_name                              |        fare        |      begin_lon      |      begin_lat      \n---------------------+------------+----------------------------------------------------------------------------+--------------------+---------------------+---------------------\n 20250729021205937   | update1    | f226d78a-a109-4b0a-b9a6-e522b75c1037-0_0-256-264_20250729021205937.parquet |  4.683013253665058 |  0.9524155201748359 |  0.5358882850214233 \n 20250729021205937   | update1    | f226d78a-a109-4b0a-b9a6-e522b75c1037-0_0-256-264_20250729021205937.parquet |  60.34776367964548 | 0.04838041157380535 |  0.8043080489965999 \n 20250729021205937   | update1    | f226d78a-a109-4b0a-b9a6-e522b75c1037-0_0-256-264_20250729021205937.parquet |  92.66658188648728 | 0.19250097971954727 | 0.29883236260196766 \n 20250729021205937   | update1    | f226d78a-a109-4b0a-b9a6-e522b75c1037-0_0-256-264_20250729021205937.parquet |  2.542602090419077 | 0.32513116973946665 |  0.5223409231279434 \n 20250729021205937   | update1    | f226d78a-a109-4b0a-b9a6-e522b75c1037-0_0-256-264_20250729021205937.parquet |     87.90342171962 |  0.6392049453784305 |   0.988424424914435 \n 20250729021205937   | update1    | f226d78a-a109-4b0a-b9a6-e522b75c1037-0_0-256-264_20250729021205937.parquet |  52.45373021236392 | 0.16908935729553864 |  0.5977389933638982 \n 20250729021205937   | update1    | f226d78a-a109-4b0a-b9a6-e522b75c1037-0_0-256-264_20250729021205937.parquet | 63.153644129279854 | 0.08661048702300178 |  0.3591754879402138 \n 20250729021205937   | update1    | f226d78a-a109-4b0a-b9a6-e522b75c1037-0_0-256-264_20250729021205937.parquet |     77.87647707179 | 0.48628479602261987 |  0.5108935227213224 \n 20250729021205937   | update1    | f226d78a-a109-4b0a-b9a6-e522b75c1037-0_0-256-264_20250729021205937.parquet |  87.60866855274442 |  0.7072482413115851 |  0.4050368427403227 \n 20250729021205937   | update1    | f226d78a-a109-4b0a-b9a6-e522b75c1037-0_0-256-264_20250729021205937.parquet |  64.50549995709942 |   0.644801818366808 | 0.27416484281668874 \n(10 rows)\n</code></pre> <p>We can also look in the MinIO UI again to see the different files that have been created. Notice in the <code>.hoodie</code> path that we have two sets of <code>deltacommit</code> files</p> <p></p> <p>We can also see, from the main <code>mor_trips_table</code> path, that we have log files. This is where Hudi stores the data that has been inserted since our table was created.</p> <p></p> <p>Let's add data in the <code>spark-shell</code> one more time, this time specifying that we want to compact the MoR table after the second commit. This means that both the changes made in this operation and in the previous \"insert\" operation will be made \"final\".</p> <pre><code>val moreUpdates = convertToStringList(dataGen.generateUpdates(100))\nval moreUpdatedData = spark.read.json(spark.sparkContext.parallelize(moreUpdates, 2));\n\nmoreUpdatedData.withColumn(\"commit_num\", lit(\"update3\")).write.format(\"hudi\").\n    options(getQuickstartWriteConfigs).\n    options(hudiOptions(morTableName, \"MERGE_ON_READ\", \"ts\")).\n    option(\"hoodie.compact.inline\", \"true\").\n    option(\"hoodie.compact.inline.max.delta.commits\", \"2\").\n    mode(Append).\n    save(s\"$basePath/$morTableName\");\n</code></pre> <p>Now when we query both tables in the Presto CLI, we see that the RO and RT MoR tables are once again in line.</p> <pre><code>select _hoodie_commit_time, commit_num, _hoodie_file_name, fare, begin_lon, begin_lat from mor_trips_table_rt order by _hoodie_commit_time;\n</code></pre> <p>For example:</p> <pre><code>presto:default&gt; select _hoodie_commit_time, commit_num, _hoodie_file_name, fare, begin_lon, begin_lat from mor_trips_table_rt order by _hoodie_commit_time;\n _hoodie_commit_time | commit_num |                             _hoodie_file_name                              |        fare        |      begin_lon       |      begin_lat      \n---------------------+------------+----------------------------------------------------------------------------+--------------------+----------------------+---------------------\n 20250729021301565   | update2    | f226d78a-a109-4b0a-b9a6-e522b75c1037-0_0-324-330_20250729021428973.parquet | 15.893431524875934 |  0.22687250146427174 | 0.01766360374572995 \n 20250729021427840   | update3    | f226d78a-a109-4b0a-b9a6-e522b75c1037-0_0-324-330_20250729021428973.parquet |  83.72026345530593 |   0.9215019802670729 |  0.3342282415053993 \n 20250729021427840   | update3    | f226d78a-a109-4b0a-b9a6-e522b75c1037-0_0-324-330_20250729021428973.parquet |  70.75181768700271 |    0.729623028385541 |  0.7839574214459966 \n 20250729021427840   | update3    | f226d78a-a109-4b0a-b9a6-e522b75c1037-0_0-324-330_20250729021428973.parquet |  9.768372582023833 |   0.4207368731538873 | 0.12793027848861438 \n 20250729021427840   | update3    | f226d78a-a109-4b0a-b9a6-e522b75c1037-0_0-324-330_20250729021428973.parquet | 16.883429270325678 |  0.06563367485824312 | 0.10377660872667782 \n 20250729021427840   | update3    | f226d78a-a109-4b0a-b9a6-e522b75c1037-0_0-324-330_20250729021428973.parquet |   60.2809650191646 |   0.8928407027516947 | 0.35471878303496784 \n 20250729021427840   | update3    | f226d78a-a109-4b0a-b9a6-e522b75c1037-0_0-324-330_20250729021428973.parquet |  30.06619331487149 |   0.2676792805368092 |  0.4255539475116672 \n 20250729021427840   | update3    | f226d78a-a109-4b0a-b9a6-e522b75c1037-0_0-324-330_20250729021428973.parquet |  88.05938803140756 | 0.015739965020097335 |  0.9567504820036522 \n 20250729021427840   | update3    | f226d78a-a109-4b0a-b9a6-e522b75c1037-0_0-324-330_20250729021428973.parquet |  60.56303585997367 |   0.2315469742599775 |  0.4746664494938815 \n 20250729021427840   | update3    | f226d78a-a109-4b0a-b9a6-e522b75c1037-0_0-324-330_20250729021428973.parquet |   71.6883114598098 |   0.9559151875258244 |  0.6027024841832427 \n(10 rows)\n</code></pre> <p>And</p> <pre><code>select _hoodie_commit_time, commit_num, _hoodie_file_name, fare, begin_lon, begin_lat from mor_trips_table_ro order by _hoodie_commit_time;\n</code></pre> <p>For example:</p> <pre><code>presto:default&gt; select _hoodie_commit_time, commit_num, _hoodie_file_name, fare, begin_lon, begin_lat from mor_trips_table_ro order by _hoodie_commit_time;\n _hoodie_commit_time | commit_num |                             _hoodie_file_name                              |        fare        |      begin_lon       |      begin_lat      \n---------------------+------------+----------------------------------------------------------------------------+--------------------+----------------------+---------------------\n 20250729021301565   | update2    | f226d78a-a109-4b0a-b9a6-e522b75c1037-0_0-324-330_20250729021428973.parquet | 15.893431524875934 |  0.22687250146427174 | 0.01766360374572995 \n 20250729021427840   | update3    | f226d78a-a109-4b0a-b9a6-e522b75c1037-0_0-324-330_20250729021428973.parquet |   71.6883114598098 |   0.9559151875258244 |  0.6027024841832427 \n 20250729021427840   | update3    | f226d78a-a109-4b0a-b9a6-e522b75c1037-0_0-324-330_20250729021428973.parquet |  9.768372582023833 |   0.4207368731538873 | 0.12793027848861438 \n 20250729021427840   | update3    | f226d78a-a109-4b0a-b9a6-e522b75c1037-0_0-324-330_20250729021428973.parquet |  30.06619331487149 |   0.2676792805368092 |  0.4255539475116672 \n 20250729021427840   | update3    | f226d78a-a109-4b0a-b9a6-e522b75c1037-0_0-324-330_20250729021428973.parquet | 16.883429270325678 |  0.06563367485824312 | 0.10377660872667782 \n 20250729021427840   | update3    | f226d78a-a109-4b0a-b9a6-e522b75c1037-0_0-324-330_20250729021428973.parquet |  88.05938803140756 | 0.015739965020097335 |  0.9567504820036522 \n 20250729021427840   | update3    | f226d78a-a109-4b0a-b9a6-e522b75c1037-0_0-324-330_20250729021428973.parquet |   60.2809650191646 |   0.8928407027516947 | 0.35471878303496784 \n 20250729021427840   | update3    | f226d78a-a109-4b0a-b9a6-e522b75c1037-0_0-324-330_20250729021428973.parquet |  60.56303585997367 |   0.2315469742599775 |  0.4746664494938815 \n 20250729021427840   | update3    | f226d78a-a109-4b0a-b9a6-e522b75c1037-0_0-324-330_20250729021428973.parquet |  83.72026345530593 |   0.9215019802670729 |  0.3342282415053993 \n 20250729021427840   | update3    | f226d78a-a109-4b0a-b9a6-e522b75c1037-0_0-324-330_20250729021428973.parquet |  70.75181768700271 |    0.729623028385541 |  0.7839574214459966 \n(10 rows)\n</code></pre> <p>In the MinIO UI, we are able to see a third set of <code>deltacommit</code>s as well as the compaction commit.</p> <p></p>"},{"location":"lab-3/#4-create-partitioned-cow-hudi-table","title":"4. Create partitioned CoW Hudi table","text":"<p>In this section we'll explore Hudi partitioned Copy-on-Write (CoW) tables. CoW tables store data using exclusively columnar file formats (e.g parquet). Updates version &amp; rewrites the files by performing a synchronous merge during write. Let's create a COW table with partitions in Spark so that we can also see how partitioning changes the directory structure of our tables. From within the <code>spark-shell</code> session from the previous sections, enter the following code in paste mode:</p> <pre><code>val cowTableName = \"cow_trips_table\"\n\nval countries = Seq(\"US\", \"IN\", \"DE\")\nval data = (1 to 500).map { i =&gt;\n  (UUID.randomUUID().toString(), s\"user_$i\", 20 + Random.nextInt(40), countries(Random.nextInt(countries.length)))\n}.toDF(\"uuid\", \"name\", \"age\", \"country\")\n\ndata.write.format(\"hudi\")\n  .options(getQuickstartWriteConfigs)\n  .options(hudiOptions(cowTableName, \"COPY_ON_WRITE\", \"age\", Some(\"country\")))\n  .mode(Overwrite)\n  .save(s\"$basePath/$cowTableName\")\n</code></pre> <p>In the MinIO UI, we can see that we now have a new table path - <code>cow_trips_table</code> - under <code>hudi_tables</code>. When we explore this path more, we can see the partition directories that have been created for this table according to our specified partition path, \"country\". Within each are the columnar data files that are associated with that partition.</p> <p></p> <p>Additionally, in the <code>.hoodie</code> directory, we can see that there is a single set of <code>commit</code> files. Notice how this differs from our MoR table, which creates <code>deltacommit</code> files.</p>"},{"location":"lab-3/#5-query-cow-table-with-presto","title":"5. Query CoW table with Presto","text":"<p>From our Presto CLI tab, we can query the new table. First verify that it has synced to the Hive metastore by running a <code>show tables</code> command:</p> <pre><code>show tables;\n</code></pre> <p>For example:</p> <pre><code>presto:default&gt; show tables;\n       Table        \n--------------------\n cow_trips_table    \n mor_trips_table_ro \n mor_trips_table_rt \n trips_table        \n(4 rows)\n</code></pre> <p>Note</p> <p>You may not have the <code>trips_table</code> depending on when you completed lab 2 and if you shut down or restarted your lakehouse cluster.</p> <p>We can then run a <code>select</code> statement:</p> <pre><code>select _hoodie_commit_time, _hoodie_partition_path, _hoodie_file_name, uuid, name, age, country from cow_trips_table limit 10;\n</code></pre> <p>For example:</p> <pre><code>presto:default&gt; select _hoodie_commit_time, _hoodie_partition_path, _hoodie_file_name, uuid, name, age, country from cow_trips_table limit 10;\n _hoodie_commit_time | _hoodie_partition_path |                             _hoodie_file_name                              |                 uuid                 |   name   | age | country \n---------------------+------------------------+----------------------------------------------------------------------------+--------------------------------------+----------+-----+---------\n 20250729001157020   | country=IN             | c74bc599-44e3-43fd-9851-6ed16c7c0b96-0_1-111-120_20250729001157020.parquet | f2a61600-ddcb-4deb-8692-43c6fc3c7460 | user_319 |  49 | IN      \n 20250729001157020   | country=IN             | c74bc599-44e3-43fd-9851-6ed16c7c0b96-0_1-111-120_20250729001157020.parquet | fd13dca9-daf1-4f61-975e-21c914326347 | user_7   |  31 | IN      \n 20250729001157020   | country=IN             | c74bc599-44e3-43fd-9851-6ed16c7c0b96-0_1-111-120_20250729001157020.parquet | ebe3334a-cca0-4abe-955d-dd7e9f27cfbd | user_133 |  34 | IN      \n 20250729001157020   | country=IN             | c74bc599-44e3-43fd-9851-6ed16c7c0b96-0_1-111-120_20250729001157020.parquet | 59fd628a-eebe-436b-852f-6c248249029d | user_160 |  36 | IN      \n 20250729001157020   | country=IN             | c74bc599-44e3-43fd-9851-6ed16c7c0b96-0_1-111-120_20250729001157020.parquet | a0141e4a-1a23-4e92-8a5f-564d64b205d7 | user_208 |  30 | IN      \n 20250729001157020   | country=IN             | c74bc599-44e3-43fd-9851-6ed16c7c0b96-0_1-111-120_20250729001157020.parquet | ba89f773-109f-4f8c-aebe-6aa237d39b31 | user_41  |  45 | IN      \n 20250729001157020   | country=IN             | c74bc599-44e3-43fd-9851-6ed16c7c0b96-0_1-111-120_20250729001157020.parquet | 01e17303-89b7-4ffc-b95f-689018acf730 | user_383 |  37 | IN      \n 20250729001157020   | country=IN             | c74bc599-44e3-43fd-9851-6ed16c7c0b96-0_1-111-120_20250729001157020.parquet | e67bcdbc-0a57-4800-a9b5-57426eb82112 | user_317 |  56 | IN      \n 20250729001157020   | country=IN             | c74bc599-44e3-43fd-9851-6ed16c7c0b96-0_1-111-120_20250729001157020.parquet | 0594e246-7b62-4ddb-b442-8759864fa270 | user_39  |  51 | IN      \n 20250729001157020   | country=IN             | c74bc599-44e3-43fd-9851-6ed16c7c0b96-0_1-111-120_20250729001157020.parquet | fa8d99e4-6100-41a1-b2c1-dec3a1688608 | user_373 |  45 | IN      \n(10 rows)\n</code></pre> <p>Notice that you can see the relevant Hudi metadata information for each row of the data.</p> <p>Note</p> <p>Note: this information is also available for the MoR tables, but we chose to omit it in the previous section for brevity.</p> <p>From here, you can experiment with adding data to our partitioned CoW table and exploring how the queries and s3 storage files change. You can also explore more advanced queries of the Hudi metadata on the MoR tables.</p>"},{"location":"lab-3/#shutdown","title":"Shutdown","text":"<p>When you're all done with the labs, to clean up your environment you can do these steps:</p> <p>In the <code>spark-shell</code> terminal, to exit the scala prompt, you enter <code>ctrl-c</code></p> <p>In the <code>presto-cli</code> terminal, to exit the presto prompt, you enter <code>ctrl-d</code></p> <p>Then, to stop all your running Docker/Podman containers, you issue:</p> <pre><code>docker compose down -v\n</code></pre> <p>Note</p> <p>Note: you need to be in the src or the src/conf folders while issuing the docker compose</p> <p>This command will stop all containers and remove the volumes.</p>"},{"location":"pre-work/","title":"Prerequisite","text":"<p>This workshop uses the Docker and Docker Compose CLI tools to set up a Presto cluster, a local REST server on top of a PostgreSQL database, and a MinIO s3 object storage instance. We recommend Podman, which is a rootless - and hence more secure - drop-in replacement for Docker. Install Podman and ensure that <code>podman</code> has been successfully <code>alias</code>'ed to <code>docker</code> in your working environment.</p>"},{"location":"pre-work/#clone-the-workshop-repository","title":"Clone the workshop repository","text":"<p>Various parts of this workshop will require the configuration files from the workshop repository. Use the following command to download the whole repository:</p> <pre><code>git clone https://github.com/IBM/presto-hudi-workshop.git\ncd presto-hudi-workshop\n</code></pre> <p>Alternatively, you can download the repository as a zip file, unzip it and change into the <code>presto-hudi-workshop</code> main directory.</p>"},{"location":"pre-work/#download-the-required-jars","title":"Download the required jars","text":"<p>We need to include some additional jars to the Spark container so that we can take advantage of Hudi and s3 functionality.</p> <p>Download the jars from the command line:</p> <pre><code>curl -sSL https://github.com/IBM/presto-hudi-workshop/releases/download/0.1.0/jars.tar.gz | tar -zxvf - -C src/conf\n</code></pre> <p>You may need to include <code>sudo</code> in the final command depending on the permissions granted in the <code>src/conf</code> directory, e.g.: <code>sudo tar -xvzf jars.tar.gz</code>.</p> <p>Alternatively, you can download the zipped jar files directly from the latest release of the repo, unzip the folder, and manually move them into the <code>src/conf/jars</code> path.</p>"},{"location":"pre-work/#optional-join-the-presto-community","title":"Optional: Join the Presto community","text":"<p>If you are working on this lab and run into issues, you can reach out on the Presto Slack <code>#presto-hudi-connector</code> channel. We'll do our best to help troubleshoot with you there! Even if you don't need any help with this workshop, we encourage you to join. Slack is the best place to meet other Presto engineers and users.</p> <p>If you're interested in contributing code or documentation to Presto, we'd love to have you! Start at the Presto GitHub repo.</p>"},{"location":"resources/ADMIN/","title":"Admin Guide","text":"<p>This section is comprised of the following steps:</p> <ol> <li>Instructor Step</li> </ol>"},{"location":"resources/ADMIN/#1-instructor-step","title":"1. Instructor Step","text":"<p>Things specific to instructors can go here.</p>"},{"location":"resources/CONTRIBUTORS/","title":"Contributors","text":""},{"location":"resources/CONTRIBUTORS/#remko-de-knikker","title":"Remko de Knikker","text":"<ul> <li>Github: remkohdev</li> <li>Twitter: @remkohdev</li> <li>LinkedIn: remkohdev</li> <li>Medium: @remkohdev</li> </ul>"},{"location":"resources/CONTRIBUTORS/#steve-martinelli","title":"Steve Martinelli","text":"<ul> <li>Github: stevemar</li> <li>Twitter: @stevebot</li> <li>LinkedIn: stevemar</li> </ul>"},{"location":"resources/MKDOCS/","title":"mkdocs examples","text":"<p>This page includes a few neat tricks that you can do with <code>mkdocs</code>. For a complete list of examples visit the mkdocs documentation.</p>"},{"location":"resources/MKDOCS/#code","title":"Code","text":"<pre><code>print(\"hello world!\")\n</code></pre>"},{"location":"resources/MKDOCS/#code-with-line-numbers","title":"Code with line numbers","text":"<pre><code>def bubble_sort(items):\n    for i in range(len(items)):\n        for j in range(len(items) - 1 - i):\n            if items[j] &gt; items[j + 1]:\n                items[j], items[j + 1] = items[j + 1], items[j]\n</code></pre>"},{"location":"resources/MKDOCS/#code-with-highlights","title":"Code with highlights","text":"<pre><code>def bubble_sort(items):\n    for i in range(len(items)):\n        for j in range(len(items) - 1 - i):\n            if items[j] &gt; items[j + 1]:\n                items[j], items[j + 1] = items[j + 1], items[j]\n</code></pre>"},{"location":"resources/MKDOCS/#code-with-tabs","title":"Code with tabs","text":"Tab Header <pre><code>#include &lt;stdio.h&gt;\n\nint main(void) {\n  printf(\"Hello world!\\n\");\n  return 0;\n}\n</code></pre> Another Tab Header <pre><code>#include &lt;iostream&gt;\n\nint main(void) {\n  std::cout &lt;&lt; \"Hello world!\" &lt;&lt; std::endl;\n  return 0;\n}\n</code></pre>"},{"location":"resources/MKDOCS/#more-tabs","title":"More tabs","text":"Windows <p>If on windows download the <code>Win32.zip</code> file and install it.</p> MacOS <p>Run <code>brew install foo</code>.</p> Linux <p>Run <code>apt-get install foo</code>.</p>"},{"location":"resources/MKDOCS/#checklists","title":"Checklists","text":"<ul> <li> Lorem ipsum dolor sit amet, consectetur adipiscing elit</li> <li> Vestibulum convallis sit amet nisi a tincidunt</li> <li> In hac habitasse platea dictumst</li> </ul>"},{"location":"resources/MKDOCS/#add-a-button","title":"Add a button","text":"<p>Launch the lab</p> <p>Visit IBM Developer</p> <p>Sign up! </p>"},{"location":"resources/MKDOCS/#call-outs","title":"Call outs","text":"<p>Tip</p> <p>You can use <code>note</code>, <code>abstract</code>, <code>info</code>, <code>tip</code>, <code>success</code>, <code>question</code> <code>warning</code>, <code>failure</code>, <code>danger</code>, <code>bug</code>, <code>quote</code> or <code>example</code>.</p> <p>Note</p> <p>A note.</p> <p>Abstract</p> <p>An abstract.</p> <p>Info</p> <p>Some info.</p> <p>Success</p> <p>A success.</p> <p>Question</p> <p>A question.</p> <p>Warning</p> <p>A warning.</p> <p>Danger</p> <p>A danger.</p> <p>Example</p> <p>A example.</p> <p>Bug</p> <p>A bug.</p>"},{"location":"resources/MKDOCS/#call-outs-with-code","title":"Call outs with code","text":"<p>Note</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p> <pre><code>def bubble_sort(items):\n    for i in range(len(items)):\n        for j in range(len(items) - 1 - i):\n            if items[j] &gt; items[j + 1]:\n                items[j], items[j + 1] = items[j + 1], items[j]\n</code></pre> <p>Nunc eu odio eleifend, blandit leo a, volutpat sapien. Phasellus posuere in sem ut cursus. Nullam sit amet tincidunt ipsum, sit amet elementum turpis. Etiam ipsum quam, mattis in purus vitae, lacinia fermentum enim.</p>"},{"location":"resources/MKDOCS/#formatting","title":"Formatting","text":"<p>In addition to the usual italics, and bold there is now support for:</p> <ul> <li>highlighted</li> <li>underlined</li> <li>strike-through</li> </ul>"},{"location":"resources/MKDOCS/#tables","title":"Tables","text":"OS or Application Username Password Windows VM <code>Administrator</code> <code>foo</code> Linux VM <code>root</code> <code>bar</code>"},{"location":"resources/MKDOCS/#emojis","title":"Emojis","text":"<p>Yes, these work.  </p>"},{"location":"resources/MKDOCS/#images","title":"Images","text":"<p>Nunc eu odio eleifend, blandit leo a, volutpat sapien</p>"},{"location":"resources/MKDOCS/#right-align-image","title":"right align image","text":"<p>Nunc eu odio eleifend, blandit leo a, volutpat sapien</p>"},{"location":"resources/RESOURCES/","title":"Additional resources","text":""},{"location":"resources/RESOURCES/#data-lakehouse","title":"Data Lakehouse","text":"<ul> <li>Hudi-Presto Slides</li> <li>What is a Data Lakehouse</li> <li>A Gentle Introduction to Data Lakehouse</li> </ul>"},{"location":"resources/RESOURCES/#presto","title":"Presto","text":"<ul> <li>Presto</li> <li>Presto Documentation</li> <li>Presto GitHub Repo</li> <li>Presto Slack</li> <li>Presto Foundation LinkedIn</li> </ul>"},{"location":"resources/RESOURCES/#hudi","title":"Hudi","text":"<ul> <li>Hudi</li> <li>Hudi Documentation</li> <li>An Overview of Hudi</li> <li>Apache Hudi GitHub Repo</li> <li>Hudi Slack</li> <li>Hudi LinkedIn</li> <li>Hudi Twitter</li> <li>Hudi YouTube Channel</li> </ul>"},{"location":"resources/RESOURCES/#minio","title":"MinIO","text":"<ul> <li>MinIO</li> <li>MinIO Documentation</li> </ul>"}]}